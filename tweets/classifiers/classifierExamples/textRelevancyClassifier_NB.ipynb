{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Text Classification ##\n",
    "\n",
    "Tha following program applies the Naive Bayes classifier provided by NLTK to input data files to determine the relevancy of a set of Twitter data. Default relevancy is to keywords relating to the Boston marathon bombing of 2013."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Author: Elizabeth Brooks\n",
    "# Date Modified: 07/09/2015\n",
    "\n",
    "# PreProcessor Directives\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.realpath('../'))\n",
    "import csv\n",
    "import yaml\n",
    "import re\n",
    "from nltk.classify import apply_features\n",
    "import random\n",
    "# Directives for twc yaml\n",
    "import twittercriteria as twc\n",
    "twc.loadCriteria()\n",
    "keyword = twc.getKeywordRegex()\n",
    "twc.clearCriteria()\n",
    "\n",
    "# Global field declarations\n",
    "current_dir = os.getcwd()\n",
    "# Set the output file path\n",
    "resultsPath = current_dir + '/relevantTweetResults.txt'\n",
    "# Initialize the training and dev data sets\n",
    "trainSet, devSet, labeledTweetDict, featureSet = []\n",
    "\n",
    "# Function to clean up tweet strings \n",
    "# by manually removing irrelevant data (not words)\n",
    "def cleanUpTweet(tweet_text):\n",
    "    # Irrelevant characters\n",
    "    twitterMarkup = ['&amp;', 'http://t.co/']\n",
    "    temp = tweet_text.lower()\n",
    "    # Use regex to create a regular expression \n",
    "    # for removing undesired characters\n",
    "    temp = re.sub('|'.join(twitterMarkup), r\"\", temp)\n",
    "    return temp\n",
    "# End cleanUpTweet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function creates a dictionary of relevent features based on the input class feature text files. Each line in the input class text files is a tweet that is separated by word, labeled and then stored in a relevancy dictionary set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to initialize the feature sets\n",
    "def initDictSet(class1_path='relevantTraining.txt', class2_path='irrelevantTraining.txt'):\n",
    "    # Loop through the txt files line by line\n",
    "    # Assign labels to tweets\n",
    "    # Two classes, relevant and irrelevant to the marathon\n",
    "    with open(current_dir + class1_path, \"r\") as relevantFile:\n",
    "        for line in relevantFile:\n",
    "            for word in line.split():\n",
    "                labeledTweetDict.append(word, 'relevant')\n",
    "    with open(current_dir + class2_path, \"r\") as irrelevantFile:\n",
    "        for line in irrelevantFile:\n",
    "            for word in line.split():\n",
    "                labeledTweetDict.append(word, 'irrelevant')\n",
    "    # Randomize the data\n",
    "    random.shuffle(labeledTweets)\n",
    "    # Close the files\n",
    "    relevantTxtFile.close()\n",
    "    irrelevantTxtFile.close()\n",
    "# End initDictSet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The extractFeatures(train_file) function is used by the trainClassifier() function to assign an input term to a feature set indicating marathon relevance. The feature set may then split into a training and development set. The training set is used by the Naive Bayes classifier provided by NLTK to train the classifier object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to extract features from tweets\n",
    "def extractFeatures(train_file):\n",
    "    # Iterate through the Twitter data csv files by tweet text\n",
    "    with open(current_dir + '/../' + train_file + '.csv') as csvfile:  \n",
    "        tweetIt = csv.DictReader(csvfile)\n",
    "        # Retrieve terms in tweets\n",
    "        for twitterData in tweetIt:\n",
    "            # Send the tweet text to the function for removing unncessary characters\n",
    "            tweetText = cleanUpTweet(twitterData['tweet_text'])\n",
    "            # Determine the feature sets\n",
    "            for word in tweetText.split():\n",
    "                featureSet = [(word, relevance) for (word, relevance) in labeledTweetDict]\n",
    "            # End for\n",
    "        # End for\n",
    "    # End with\n",
    "# End extractFeatures\n",
    "\n",
    "# Function for training the classifier\n",
    "def trainClassifier():   \n",
    "    # Establish the training set\n",
    "    # Add dev set assignment here\n",
    "    trainSet = featuresSet\n",
    "\n",
    "    # Train the Naive Bayes (NB) classifier\n",
    "    classifierNB = nltk.NaiveBayesClassifier.train(trainSet)\n",
    "# End trainClassifyData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below classifyCSV(test_file) function cleans and classifys a data set in CSV file format, while the isRelevant(tweet_text) function may be used to classify a single tweet string. However, before the function may be called, the classifier must be trained using the previously defined functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to classify input test data, csv file format\n",
    "def classifyCSV(test_file):    \n",
    "    # Classify input test data\n",
    "    # Create object for writting to a text file\n",
    "    tweetResultsFile = open(resultsPath, \"w\")\n",
    "    # Iterate through the Twitter data csv files by tweet text\n",
    "    with open(current_dir + '/../' + test_file + '.csv') as csvfile:  \n",
    "        tweetIt = csv.DictReader(csvfile)\n",
    "        # Retrieve terms in tweets\n",
    "        for twitterData in tweetIt:\n",
    "            # Send the tweet text to the function for removing unncessary characters\n",
    "            tweetText = cleanUpTweet(twitterData['tweet_text'])\n",
    "            # Send the results of the classifier to a txt file\n",
    "            tweetResultsFile.write(classifierNB.classify(tweetText))\n",
    "        # End for\n",
    "    # End with\n",
    "    # Close file\n",
    "    tweetResultsFile.close()\n",
    "# End classifyCSV\n",
    "\n",
    "# Function to classify input cleaned tweet txt\n",
    "def isRelevant(tweet_text):\n",
    "    # Return the use of the classifier\n",
    "    return classifierNB.classify(tweet_text)\n",
    "# End isRelevant\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main() method may be used to run the script and classify a set okf Twitter data by requesting user input of not only the training and test data files, but also the two (at this time) classes. Where the classes are text files of \"labeled\" (organized by file) tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The main method\n",
    "def main():\n",
    "    # Request user input of text class files\n",
    "    inputClassFile1 = raw_input(\"Enter the first class feature set txt file name...\\nEx: relevantTraining.txt\")\n",
    "    inputClassFile2 = raw_input(\"Enter the second class feature set txt file name...\\nEx: irrelevantTraining.txt\")\n",
    "\n",
    "    # Initialize the classifier dictionary based on relevant features\n",
    "    initDictSet(inputClassFile1, inputClassFile2)\n",
    "\n",
    "    # Request user input of the file name of train/dev data to be processed\n",
    "    inputTrainFile = raw_input(\"Enter training data set csv file name...\\nEx: cleaned_geo_tweets_Apr_12_to_22\")\n",
    "    # Request file name of data to be classified\n",
    "    inputTestFile = raw_input(\"Enter test data set csv file name...\\nEx: cleaned_geo_tweets_Apr_12_to_22\")\n",
    "    \n",
    "    # Extract features and train the NB classifier using input training data\n",
    "    extractFeatures(inputTrainFile)\n",
    "    trainClassifier()\n",
    "    \n",
    "    # Classify the input test data, csv file format\n",
    "    classifyCSV(inputTestFile)\n",
    "# End main\n",
    "\n",
    "# Run the script via the main method\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    \n",
    "# End script"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
